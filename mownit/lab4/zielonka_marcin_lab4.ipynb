{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metody obliczeniowe w nauce i technice\n",
    "\n",
    "## Laboratorium 4 - Singular Value Decomposition\n",
    "\n",
    "### Sprawozdanie sporządził: Marcin Zielonka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wstęp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do realizacji zadań skorzystam z gotowych funkcjonalności zawartych w bibliotekach:\n",
    "* `numpy`\n",
    "* `math`\n",
    "* `nltk`\n",
    "* `num2words`\n",
    "* `re`\n",
    "* `scipy`\n",
    "* `functools`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "from num2words import num2words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1: Wyszukiwarka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Przygotuj duży (> 1000 elementów) zbiór dokumentów tekstowych w języku angielskim (np. wybrany korpus tekstów, podzbiór artykułów Wikipedii, zbiór dokumentów HTML uzyskanych za pomocą *Web crawlera*, zbiór rozdziałów wyciętych z różnych książek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do realizacji zadań wykorzystam zbiór 1100 wiadomości tekstowych w języku angielskim (źródło: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/). Każda wiadomość umieszczona jest w osobnej linii w pliku `data.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1100\n"
     ]
    }
   ],
   "source": [
    "file = open('data.txt', 'r')\n",
    "\n",
    "documents = np.array(list(file))\n",
    "\n",
    "print(f'Number of documents: {len(documents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Określ słownik słów kluczowych (termów) potrzebny do wyznaczenia wektorów cech *bag-of-words* (indeksacja). Przykładowo zbiorem takim może być unia wszystkich słów występujących we wszystkich tekstach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby wyszukiwarka działała wydajnie, każdy z tekstów został wstępnie odpowiednio przetworzony:\n",
    "\n",
    "* zamieniono wszystkie litery na małe\n",
    "* usunięto znaki specjalne\n",
    "* zamieniono liczby na ich odpowiedniki w jęz. angielskim\n",
    "* usunięto powtarzające się spacje\n",
    "* usunięto słowa pełniące jedynie rolę pomocniczą (np. aby zachować reguły gramatyki) - takie jak *a*, *the*, itd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do tego celu wykorzystano *regular expressions* oraz gotowe funkcjonalności zawarte w bibliotekach `nltk` oraz `num2words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_document(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub('_', '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        word = lemmatizer.lemmatize(word, pos='v')\n",
    "        \n",
    "        try:\n",
    "            word = num2words(int(word))\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        \n",
    "        words.append(word)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary(documents):\n",
    "    dict = {}\n",
    "    \n",
    "    all_words = reduce(list.__add__, list(map(prepare_document, documents)))\n",
    "    \n",
    "    for word in all_words:\n",
    "        if word not in dict.keys():\n",
    "            dict[word] = 0\n",
    "        dict[word] += 1\n",
    "    \n",
    "    all_words = list(dict.keys())\n",
    "    all_words.sort()\n",
    "    \n",
    "    return dict, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, all_words = make_dictionary(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Dla każdego dokumentu $j$ wyznacz wektor cech *bag-of-words* $\\mathbf{d}_j$ zawierający częstości występowania poszczególnych słów (termów) w tekście."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(words, all_words):\n",
    "    bag_of_words = [0] * len(all_words)\n",
    "    words_amount = len(words)\n",
    "    \n",
    "    for word in words:\n",
    "        idx = all_words.index(word)\n",
    "        bag_of_words[idx] += (1 / words_amount)\n",
    "\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bags_of_words(documents, all_words):\n",
    "    return list(map(lambda words: bag_of_words(words, all_words), list(map(prepare_document, documents))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bags_of_words = bags_of_words(documents, all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Zbuduj rzadką macierz wektorów cech term-by-document matrix w której wektory cech ułożone są kolumnowo $A_{m×n}=[\\mathbf{d}_1|\\mathbf{d}_2|...|\\mathbf{d}_n]$ ($m$ jest liczbą termów w słowniku, a $n$ liczbą dokumentów)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_by_document_matrix(bags_of_words):\n",
    "    matrix = []\n",
    "    \n",
    "    for bag in bags_of_words:\n",
    "        matrix.append(bag)\n",
    "\n",
    "    return matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_by_document_matrix = term_by_document_matrix(bags_of_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Przetwórz wstępnie otrzymany zbiór danych mnożąc elementy *bag-of-words* przez *inverse document frequency*. Operacja ta pozwoli na redukcję znaczenia często występujących słów.\n",
    "\n",
    "$$IDF(w)=log\\frac{N}{n_w}$$\n",
    "\n",
    "   gdzie $n_w$ jest liczbą dokumentów, w których występuje słowo $w$, a $N$ jest całkowitą\n",
    "liczbą dokumentów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_by_idf(term_by_document_matrix):\n",
    "    documents_size = len(term_by_document_matrix)\n",
    "    words_frequency = []\n",
    "    \n",
    "    for idx in range(len(term_by_document_matrix[0])):\n",
    "        occurences = 0\n",
    "        \n",
    "        for doc in term_by_document_matrix:\n",
    "           if doc[idx] > 0:\n",
    "            occurences += 1\n",
    "        \n",
    "        words_frequency.append(math.log(documents_size / occurences))\n",
    "        \n",
    "    IDF = []\n",
    "    words_frequency = np.array(words_frequency)\n",
    "    for doc in np.array(term_by_document_matrix):\n",
    "        IDF.append(doc * words_frequency)\n",
    "    \n",
    "    return IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF = multiply_by_idf(term_by_document_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Napisz program pozwalający na wprowadzenie zapytania (w postaci sekwencji słów) przekształcanego następnie do reprezentacji wektorowej $\\mathbf{q}$ (*bag-of-words*). Program ma zwrócić $k$ dokumentów najbardziej zbliżonych do podanego zapytania $\\mathbf{q}$. Użyj korelacji między wektorami jako miary podobieństwa.\n",
    "\n",
    "$$\n",
    "cos\\theta_j=\n",
    "\\frac{\\mathbf{q}^T\\mathbf{d}_j}{\\lVert\\mathbf{q}\\rVert\\lVert\\mathbf{d}_j\\rVert}=\n",
    "\\frac{\\mathbf{q}^T\\mathbf{A}\\mathbf{e}_j}{\\lVert\\mathbf{q}\\rVert\\lVert\\mathbf{A}\\mathbf{e}_j\\rVert}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(search_text, k, IDF, all_words):\n",
    "    number_of_words = len(all_words)\n",
    "    \n",
    "    search_words = prepare_document(search_text)\n",
    "    \n",
    "    q = np.array(bag_of_words(search_words, all_words))\n",
    "    q_norm = np.linalg.norm(q)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for idx, doc in enumerate(np.array(IDF)):\n",
    "        doc_val = (q.T @ doc) / (q_norm * np.linalg.norm(doc))\n",
    "        result.append((idx, doc_val))\n",
    "    \n",
    "    result.sort(key=lambda doc_tuple: doc_tuple[1], reverse=True)\n",
    "    \n",
    "    for doc_idx in [doc[0] for doc in result[:k]]:\n",
    "        print(f'Message {doc_idx}:')\n",
    "        print(documents[doc_idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 73:\n",
      "I'm really not up to it still tonight babe\n",
      "\n",
      "Message 333:\n",
      "Huh so late... Fr dinner?\n",
      "\n",
      "Message 498:\n",
      "Huh so early.. Then ü having dinner outside izzit?\n",
      "\n",
      "Message 243:\n",
      "K, I might come by tonight then if my class lets out early\n",
      "\n",
      "Message 458:\n",
      "Probably gonna be here for a while, see you later tonight &lt;)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "query(\"dinner tonight?\", 5, IDF, all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Zastosuj normalizację wektorów cech $\\mathbf{d}_j$ i wektora $\\mathbf{q}$, tak aby miały one długość 1. Użyj zmodyfikowanej miary podobieństwa otrzymując\n",
    "\n",
    "$$|\\mathbf{q}^T\\mathbf{A}|=[|cos\\theta_1|,|cos\\theta_2|,...,|cos\\theta_n|]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_norm(search_text, k, term_by_document_matrix, all_words):\n",
    "    number_of_words = len(all_words)\n",
    "    \n",
    "    search_words = prepare_document(search_text)\n",
    "    \n",
    "    q = np.array(bag_of_words(search_words, all_words))\n",
    "    q = np.array([q / np.linalg.norm(q)])\n",
    "    \n",
    "    A = np.array([e / np.linalg.norm(e) for e in term_by_document_matrix])\n",
    "    \n",
    "    M = A @ q.T\n",
    "    result = []\n",
    "    \n",
    "    for idx, val in enumerate(M):\n",
    "        result.append((idx, val))\n",
    "    \n",
    "    result.sort(key=lambda doc_tuple: doc_tuple[1], reverse=True)\n",
    "    \n",
    "    for doc_idx in [doc[0] for doc in result[:k]]:\n",
    "        print(f'Message {doc_idx}:')\n",
    "        print(documents[doc_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 198:\n",
      "Sorry battery died, yeah I'm here\n",
      "\n",
      "Message 761:\n",
      "I wonder if your phone battery went dead ? I had to tell you, I love you babe\n",
      "\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 381:\n",
      "Sorry, my battery died, I can come by but I'm only getting a gram for now, where's your place?\n",
      "\n",
      "Message 557:\n",
      "I'm gonna say no. Sorry. I would but as normal am starting to panic about time. Sorry again! Are you seeing on Tuesday?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "query_norm(\"sorry battery dead\", 5, term_by_document_matrix, all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. W celu usunięcia szumu z macierzy A zastosuj SVD i low rank approximation otrzymując\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\approx\\mathbf{A}_k=\n",
    "\\mathbf{U}_k\\mathbf{D}_k\\mathbf{V}_k^T=\n",
    "[\\mathbf{u}_1|...|\\mathbf{u}_k]\n",
    "\\begin{bmatrix}\n",
    "\\sigma_1 & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & \\sigma_k\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{v}_1^T \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{v}_k^T\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sum_{i=1}^{k}\\sigma_i\\mathbf{u}_i\\mathbf{v}_i^T\n",
    "$$\n",
    "\n",
    "oraz nową miarę podobieństwa\n",
    "\n",
    "$$\n",
    "cos\\Phi_j=\n",
    "\\frac{\\mathbf{q}^T\\mathbf{A}_k\\mathbf{e}_j}{\\lVert\\mathbf{q}\\rVert\\lVert\\mathbf{A}_k\\mathbf{e}_j\\rVert}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_approximation(IDF, k):\n",
    "    U, S, V = scipy.sparse.linalg.svds(np.array(IDF), k=k)\n",
    "    return U @ np.diag(S) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_approximations = []\n",
    "for k in range(5,101,5):\n",
    "    lr_approximations.append(low_rank_approximation(IDF, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Porównaj działanie programu bez usuwania szumu i z usuwaniem szumu. Dla jakiej wartości $k$ wyniki wyszukiwania są najlepsze (subiektywnie). Zbadaj wpływ przekształcenia IDF na wyniki wyszukiwania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results without removing noise:\n",
      "\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 289:\n",
      "Sorry, I'll call later\n",
      "\n",
      "Message 65:\n",
      "Sorry, I'll call later\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Results without removing noise:\\n')\n",
    "query(\"sorry battery dead\", 3, IDF, all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with removing noise:\n",
      "\n",
      "\n",
      "=============== k: 5 ===============\n",
      "Message 117:\n",
      "Sir, Waiting for your mail.\n",
      "\n",
      "Message 59:\n",
      "U can call me now...\n",
      "\n",
      "Message 385:\n",
      "10 min later k...\n",
      "\n",
      "=============== k: 10 ===============\n",
      "Message 59:\n",
      "U can call me now...\n",
      "\n",
      "Message 66:\n",
      "K. Did you call me just now ah?\n",
      "\n",
      "Message 926:\n",
      "Please da call me any mistake from my side sorry da. Pls da goto doctor.\n",
      "\n",
      "=============== k: 15 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 534:\n",
      "sorry, no, have got few things to do. may be in pub later.\n",
      "\n",
      "Message 904:\n",
      "That's fine, I'll bitch at you about it later then\n",
      "\n",
      "=============== k: 20 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 534:\n",
      "sorry, no, have got few things to do. may be in pub later.\n",
      "\n",
      "Message 904:\n",
      "That's fine, I'll bitch at you about it later then\n",
      "\n",
      "=============== k: 25 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 904:\n",
      "That's fine, I'll bitch at you about it later then\n",
      "\n",
      "Message 534:\n",
      "sorry, no, have got few things to do. may be in pub later.\n",
      "\n",
      "=============== k: 30 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 904:\n",
      "That's fine, I'll bitch at you about it later then\n",
      "\n",
      "Message 534:\n",
      "sorry, no, have got few things to do. may be in pub later.\n",
      "\n",
      "=============== k: 35 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 904:\n",
      "That's fine, I'll bitch at you about it later then\n",
      "\n",
      "Message 534:\n",
      "sorry, no, have got few things to do. may be in pub later.\n",
      "\n",
      "=============== k: 40 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 904:\n",
      "That's fine, I'll bitch at you about it later then\n",
      "\n",
      "Message 534:\n",
      "sorry, no, have got few things to do. may be in pub later.\n",
      "\n",
      "=============== k: 45 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 904:\n",
      "That's fine, I'll bitch at you about it later then\n",
      "\n",
      "Message 534:\n",
      "sorry, no, have got few things to do. may be in pub later.\n",
      "\n",
      "=============== k: 50 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 904:\n",
      "That's fine, I'll bitch at you about it later then\n",
      "\n",
      "Message 1042:\n",
      "Jus ans me lar. U'll noe later.\n",
      "\n",
      "=============== k: 55 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 904:\n",
      "That's fine, I'll bitch at you about it later then\n",
      "\n",
      "Message 1042:\n",
      "Jus ans me lar. U'll noe later.\n",
      "\n",
      "=============== k: 60 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 904:\n",
      "That's fine, I'll bitch at you about it later then\n",
      "\n",
      "Message 65:\n",
      "Sorry, I'll call later\n",
      "\n",
      "=============== k: 65 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 904:\n",
      "That's fine, I'll bitch at you about it later then\n",
      "\n",
      "Message 379:\n",
      "Sorry, I'll call later\n",
      "\n",
      "=============== k: 70 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 602:\n",
      "Sorry, I'll call later\n",
      "\n",
      "Message 959:\n",
      "Sorry, I'll call later\n",
      "\n",
      "=============== k: 75 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 602:\n",
      "Sorry, I'll call later\n",
      "\n",
      "Message 65:\n",
      "Sorry, I'll call later\n",
      "\n",
      "=============== k: 80 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 65:\n",
      "Sorry, I'll call later\n",
      "\n",
      "Message 959:\n",
      "Sorry, I'll call later\n",
      "\n",
      "=============== k: 85 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 379:\n",
      "Sorry, I'll call later\n",
      "\n",
      "Message 289:\n",
      "Sorry, I'll call later\n",
      "\n",
      "=============== k: 90 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 65:\n",
      "Sorry, I'll call later\n",
      "\n",
      "Message 190:\n",
      "Sorry, I'll call later\n",
      "\n",
      "=============== k: 95 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 534:\n",
      "sorry, no, have got few things to do. may be in pub later.\n",
      "\n",
      "Message 65:\n",
      "Sorry, I'll call later\n",
      "\n",
      "=============== k: 100 ===============\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 289:\n",
      "Sorry, I'll call later\n",
      "\n",
      "Message 65:\n",
      "Sorry, I'll call later\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Results with removing noise:\\n\\n')\n",
    "\n",
    "for idx, IDF in enumerate(lr_approximations):\n",
    "    print(f'=============== k: {idx * 5 + 5} ===============')\n",
    "    query(\"sorry battery dead\", 3, IDF, all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyszukiwanie z wstępnym usuwaniem szumu daje bardzo zbliżone rezulaty do wyszukiwania bez usuwania szumu dla wartości $k$ powyżej wartości 15/20. Poniżej tej wartości otrzymane wyniki zdają się być losowe i często nie zawierają żadnych z wprowadzonych słów kluczowych.\n",
    "\n",
    "Dla wyższych wartości $k$ (rzędu 60-70) algorytm przeszukiwania daje najbardziej zbliżone do wyszukiwania rezultaty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wpływ przekształcenia IDF na wyniki wyszukiwania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 198:\n",
      "Sorry battery died, yeah I'm here\n",
      "\n",
      "Message 761:\n",
      "I wonder if your phone battery went dead ? I had to tell you, I love you babe\n",
      "\n",
      "Message 768:\n",
      "I am sorry it hurt you.\n",
      "\n",
      "Message 381:\n",
      "Sorry, my battery died, I can come by but I'm only getting a gram for now, where's your place?\n",
      "\n",
      "Message 557:\n",
      "I'm gonna say no. Sorry. I would but as normal am starting to panic about time. Sorry again! Are you seeing on Tuesday?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "query(\"sorry battery dead\", 5, term_by_document_matrix, all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przekształcenie IDF ma znaczący wpływ na wyniki wyszukiwania, ponieważ dzięki tej transformacji wyszukiwarka jest bardziej wyczulona na rzadziej występujące słowa, a mniej na te popularniejsze (w tym przykładzie na słowo *sorry*). Co za tym idzie jest większe prawdopodobieństwo, że otrzymamy bardziej interesujące nas wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
